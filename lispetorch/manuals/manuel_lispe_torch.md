# Manuel LispE PyTorch - Guide Complet

Un guide complet pour l'utilisation de la librairie LispE PyTorch, offrant une int√©gration compl√®te entre le langage LispE et l'√©cosyst√®me PyTorch pour l'apprentissage automatique.

## Table des Mati√®res

1. [Introduction](#introduction)
2. [Installation et Configuration](#installation-et-configuration)
3. [Types de Donn√©es](#types-de-donn√©es)
4. [Op√©rations de Base sur les Tenseurs](#op√©rations-de-base-sur-les-tenseurs)
5. [Mod√®les et R√©seaux de Neurones](#mod√®les-et-r√©seaux-de-neurones)
6. [Chargement de Mod√®les Hugging Face](#chargement-de-mod√®les-hugging-face)
7. [Fine-tuning avec LoRA](#fine-tuning-avec-lora)
8. [G√©n√©ration de Texte](#g√©n√©ration-de-texte)
9. [Tokenisation](#tokenisation)
10. [Flash Attention](#flash-attention)
11. [Optimisation et Quantification](#optimisation-et-quantification)
12. [Tutoriel Complet d'Entra√Ænement LoRA](#tutoriel-complet-dentra√Ænement-lora)
13. [Exemples Pratiques](#exemples-pratiques)

## Introduction

LispE PyTorch est une librairie qui int√®gre les capacit√©s de PyTorch dans le langage de programmation fonctionnel LispE. Elle permet de cr√©er, entra√Æner et d√©ployer des mod√®les d'apprentissage automatique avec la syntaxe √©l√©gante de LispE.

### Caract√©ristiques Principales

- **Int√©gration Native PyTorch** : Acc√®s direct aux tenseurs et op√©rations PyTorch
- **Support GPU** : CUDA pour NVIDIA et Metal Performance Shaders (MPS) pour Apple Silicon
- **Mod√®les Hugging Face** : Chargement et ex√©cution de mod√®les pr√©-entra√Æn√©s
- **LoRA Fine-tuning** : Fine-tuning efficace avec Low-Rank Adaptation
- **Flash Attention** : Attention m√©moire-efficace pour longues s√©quences
- **G√©n√©ration de Texte** : Syst√®me complet avec multiples strat√©gies d'√©chantillonnage
- **Quantification** : Support FP16 et INT8 pour l'optimisation des mod√®les

## Installation et Configuration

### Chargement de la Librairie

```lisp
; Charger la librairie PyTorch
(use 'lispe_torch)

; V√©rifier la disponibilit√© GPU
(if (torch_cuda_is_available)
    (println "‚úì CUDA disponible")
    (if (torch_mps_is_available) 
        (println "‚úì MPS (Apple Silicon) disponible")
        (println "‚Ä¢ Utilisation du CPU")))
```

### Configuration de Base

```lisp
; D√©finir le device par d√©faut
(setq device (cond 
    ((torch_cuda_is_available) "cuda")
    ((torch_mps_is_available) "mps")
    (true "cpu")))

(println "Device s√©lectionn√©:" device)
```

## Types de Donn√©es

### Types LispE Optimis√©s pour PyTorch

LispE fournit des types de donn√©es optimis√©s qui s'int√®grent efficacement avec PyTorch :

```lisp
; Types de listes optimis√©es (zero-copy)
(setq entiers (integers 1 2 3 4 5))       ; Liste d'entiers
(setq flotants (floats 1.0 2.0 3.0 4.0))  ; Liste de float
(setq nombres (numbers 1.0 2.0 3.0))      ; Liste de double
(setq courts (shorts 1 2 3))              ; Liste de short

; G√©n√©rateurs de s√©quences
(setq seq1 (iota0 5))    ; [0, 1, 2, 3, 4]
(setq seq2 (iota 5))     ; [1, 2, 3, 4, 5]
```

### Types PyTorch dans LispE

```lisp
; Types principaux
; tensor_create    - Encapsule torch::Tensor
; torch_model     - Encapsule les mod√®les PyTorch
; torch_optimizer - Encapsule les optimiseurs
; torch_tokenizer - Interface de tokenisation
```

## Op√©rations de Base sur les Tenseurs

### Cr√©ation de Tenseurs

```lisp
; Cr√©ation √† partir de listes LispE
(setq data (floats 1.0 2.0 3.0 4.0))
(setq tensor (tensor_create data))

; Tenseurs avec formes sp√©cifiques
(setq zeros_tensor (tensor_zeros (integers 3 4)))
(setq ones_tensor (tensor_ones (integers 2 3)))
(setq random_tensor (tensor_randn (integers 4 4)))

; Tenseurs avec device sp√©cifique
(setq cuda_tensor (torch_to_cuda (tensor_randn (integers 2 2))))
(setq mps_tensor (torch_to_mps (tensor_randn (integers 2 2))))
```

### Op√©rations Math√©matiques

```lisp
; Op√©rations de base
(setq a (tensor_randn (integers 3 3)))
(setq b (tensor_randn (integers 3 3)))

(setq somme (tensor_add a b))
(setq produit (tensor_matmul a b))
(setq transposee (tensor_transpose a 0 1))

; Fonctions d'activation
(setq tensor (tensor_randn (integers 10)))
(setq relu_result (tensor_relu tensor))
(setq sigmoid_result (tensor_sigmoid tensor))
(setq softmax_result (tensor_softmax tensor -1))

; Op√©rations par √©l√©ment
(setq div_result (tensor_div_scalar tensor 2.0))
(setq shape (tensor_shape tensor))
(setq item_value (tensor_item (tensor_select tensor 0)))
```

### Manipulation de Formes

```lisp
; Obtenir les dimensions
(setq shape (tensor_shape tensor))
(println "Forme du tensor:" shape)

; Redimensionner
(setq reshaped (tensor_reshape tensor (integers 2 2)))
(setq squeezed (tensor_squeeze tensor))
(setq unsqueezed (tensor_unsqueeze tensor 0))
```

## Mod√®les et R√©seaux de Neurones

### Cr√©ation de Mod√®les Simples

```lisp
; Cr√©er des couches individuelles PyTorch
(setq linear1 (torch_linear 784 128))    ; couche lin√©aire input->hidden
(setq linear2 (torch_linear 128 10))     ; couche lin√©aire hidden->output

; Cr√©er des donn√©es d'exemple
(setq input_data (tensor_randn (integers 32 784)))  ; batch_size=32, features=784

; Propagation avant manuelle
(setq h1 (torch_linear_forward linear1 input_data))  ; premi√®re couche
(setq h1_relu (tensor_relu h1))                       ; activation ReLU
(setq output (torch_linear_forward linear2 h1_relu)) ; deuxi√®me couche

; Autres composants disponibles
(setq embedding (torch_embedding 1000 128))   ; vocabulary=1000, dim=128
(setq layer_norm (torch_layer_norm 128))      ; normalisation
(setq attention (torch_multihead_attention 128 8))  ; attention multi-t√™tes
```

### Chargement de Mod√®les Pr√©-entra√Æn√©s

```lisp
; Pour charger des mod√®les pr√©-entra√Æn√©s (diff√©rent de cr√©er des mod√®les)
(setq model_path "path/to/huggingface/model")
(setq config (dictionary "device" "mps"))
(setq model_id (torch_hf_load_model model_path config))

; Forward pass avec mod√®le HuggingFace
(setq input_tokens (tensor_create (integers 15496 318 257)))
(setq input_batch (tensor_unsqueeze input_tokens 0))
(setq logits (torch_hf_forward model_id input_batch))
```

### Blocs Transformer

```lisp
; Cr√©er un bloc Transformer
(setq transformer_block (torch_transformer_block 512 8 2048))

; Forward pass avec masque  
(setq input_seq (tensor_randn (integers 32 128 512)))  ; [batch, seq, dim]
(setq mask (tensor_ones (integers 32 128 128)))        ; attention mask
(setq output (torch_transformer_forward transformer_block input_seq mask))
```

### Entra√Ænement

```lisp
; Cr√©er un optimiseur
(setq learning_rate 0.001)
(setq optimizer (torch_adamw_optimizer learning_rate))
(torch_optimizer_add_params optimizer mlp)

; Boucle d'entra√Ænement
(loop i 100
    ; Forward pass
    (setq predictions (torch_forward mlp input_data))
    
    ; Calcul de la loss
    (setq loss (torch_mse_loss predictions targets))
    
    ; Backward pass
    (torch_optimizer_zero_grad optimizer)
    (torch_backward loss)
    (torch_optimizer_step optimizer)
    
    ; Affichage du progr√®s
    (if (== (% i 10) 0)
        (println "Epoch" i "Loss:" (tensor_item loss)))
)
```

## Chargement de Mod√®les Hugging Face

### Configuration et Chargement

```lisp
; Configuration compl√®te pour le chargement HuggingFace
(setq model_path "/path/to/model")
(setq config (dictionary 
    ; === PARAM√àTRES DE BASE ===
    "device" "mps"                    ; Device : "cuda", "mps", "cpu"
    
    ; === PARAM√àTRES DE S√âQUENCE ===
    "max_seq_len" 2048                ; Longueur maximale de s√©quence support√©e
    "rope_scaling" 1.0                ; Facteur d'√©chelle pour RoPE (Rotary Position Embedding)
    
    ; === PARAM√àTRES DE G√âN√âRATION ===
    "temperature" 0.7                 ; Temp√©rature pour le sampling (0.1-2.0)
    "top_p" 0.9                       ; Nucleus sampling - probabilit√© cumulative
    "top_k" 50                        ; Top-K sampling - nombre de tokens consid√©r√©s
    "repetition_penalty" 1.1          ; P√©nalit√© de r√©p√©tition (1.0 = pas de p√©nalit√©)
    
    ; === PARAM√àTRES DE CACHE ===
    "use_kv_cache" true               ; Activer le cache Key-Value pour g√©n√©ration
    "max_cache_len" 4096              ; Taille maximale du cache (fen√™tre glissante)
    
    ; === PARAM√àTRES AVANC√âS ===
    "manual_attention" false          ; Mode attention manuelle (debugging/contr√¥le fin)
))

; Charger le mod√®le avec configuration compl√®te
(setq model_id (torch_hf_load_model model_path config))
(println "‚úì Mod√®le charg√© avec ID:" model_id)
```

### Inf√©rence avec Cache KV

```lisp
; Activer le cache KV pour une g√©n√©ration efficace
(setq context_id (torch_hf_enable_kv_cache model_id true))

; Pr√©parer les tokens d'entr√©e
(setq input_tokens (tensor_create (integers 15496 318 257)))  ; "This is a"
(setq input_batch (tensor_unsqueeze input_tokens 0))         ; Ajouter dimension batch

; Forward pass (utilise automatiquement le cache)
(setq logits (torch_hf_forward model_id input_batch context_id))

; Extraire les logits du dernier token
(setq last_logits (tensor_select logits 1 -1))
(setq next_token (tensor_argmax last_logits -1))
```

## Fine-tuning avec LoRA

### Initialisation LoRA

```lisp
; Charger le mod√®le avec support LoRA
(setq model_name "llama_lora")
(torch_hf_load_model_lora 
    model_path 
    config 
    model_name)

; Configuration LoRA
(setq lora_config (dictionary
    "rank" 16
    "alpha" 32.0
    "target_modules" (strings "q_proj" "k_proj" "v_proj" "o_proj")
    "dtype" "float16"
))

; Initialiser les adaptateurs LoRA
(torch_hf_lora_init 
    model_name 
    lora_config)
```

### Entra√Ænement LoRA

```lisp
; Cr√©er optimiseur pour param√®tres LoRA uniquement  
(setq lora_params (torch_hf_lora_get_parameters model_name))
(setq learning_rate 2e-4)
(setq optimizer (torch_adamw_optimizer learning_rate))
(torch_optimizer_add_params optimizer lora_params)

; Boucle d'entra√Ænement
(loop epoch 3
    (println "Epoch" (+ epoch 1))
    
    ; Pour chaque batch de donn√©es
    (loop batch_idx num_batches
        ; Pr√©parer les donn√©es
        (setq input_ids (get_batch_data batch_idx))
        
        ; Forward pass avec LoRA
        (setq logits (torch_hf_forward model_name input_ids))
        
        ; Calcul de la loss
        (setq loss (calculate_lm_loss logits input_ids))
        
        ; Backward pass
        (torch_optimizer_zero_grad optimizer)
        (torch_backward loss)
        (torch_optimizer_step optimizer)
        
        ; Logging
        (if (== (% batch_idx 10) 0)
            (println "  Batch" batch_idx "Loss:" (tensor_item loss)))
    )
    
    ; Sauvegarder les adaptateurs LoRA
    (torch_hf_lora_save model_name (+ output_dir "/epoch_" epoch))
)
```

## G√©n√©ration de Texte

### Classe Model pour G√©n√©ration

Voici un exemple d'impl√©mentation de classe pour la g√©n√©ration de texte :

```lisp
; Classe Model pour l'inf√©rence
(class@ Model (model_path config tokenizer init)
    (defun configure()
        ; Initialiser le mod√®le HuggingFace
        (setqi model_id (torch_hf_load_model model_path config))
        (setqi temperature 0.7)
        (setqi top_p 0.9)
        (setqi max_tokens 100)
    )
    
    (defun generate(prompt max_length)
        ; Encoder le prompt
        (setq prompt_tokens (tokenizer (encode prompt)))
        (setq generated_tokens (clone prompt_tokens))
        
        ; Activer le cache KV pour performance
        (setq context_id (torch_hf_enable_kv_cache model_id true))
        
        ; G√©n√©ration token par token
        (setq current_input (tensor_unsqueeze (tensor_create prompt_tokens) 0))
        
        (loop i max_length
            ; Forward pass
            (setq logits (torch_hf_forward model_id current_input context_id))
            (setq last_logits (tensor_select logits 1 -1))
            (setq last_logits (tensor_select last_logits 0 0))
            
            ; Application de la temp√©rature
            (setq scaled_logits (tensor_div_scalar last_logits temperature))
            
            ; Sampling avec top-p (nucleus sampling)
            (setq probs (tensor_softmax scaled_logits -1))
            (setq next_token (tensor_multinomial probs 1 true))
            (setq next_token_id (tensor_item next_token))
            
            ; Afficher le token g√©n√©r√©
            (print (tokenizer (decode (integers next_token_id))))
            
            ; Ajouter √† la s√©quence
            (push generated_tokens next_token_id)
            
            ; V√©rifier token de fin
            (check (== next_token_id (tokenizer (eos_id)))
                (break)
            )
            
            ; Pr√©parer input suivant (seulement le nouveau token)
            (setq current_input (tensor_unsqueeze (tensor_create (integers next_token_id)) 0))
        )
        
        (println)  ; Nouvelle ligne
        generated_tokens
    )
)
```

### G√©n√©ration Avanc√©e avec Param√®tres

```lisp
; G√©n√©rateur avec contr√¥le fin
(setq generator_config (dictionary
    "temperature" 0.8
    "top_k" 50
    "top_p" 0.95
    "repetition_penalty" 1.1
    "max_length" 200
    "do_sample" true
))

; G√©n√©ration avec configuration compl√®te
(defun generate_with_config(model_path prompt_tokens eos_id)
    ; Exemples avec diff√©rentes strat√©gies de sampling
    
    ; 1. G√©n√©ration par d√©faut (sampling simple)
    (setq result_default (torch_hf_generate model_path prompt_tokens eos_id 100))
    
    ; 2. G√©n√©ration gloutonne (d√©terministe)
    (setq greedy_options (dictionary "greedy" true))
    (setq result_greedy (torch_hf_generate model_path prompt_tokens eos_id 100 greedy_options))
    
    ; 3. Top-K sampling (diversit√© contr√¥l√©e)
    (setq topk_options (dictionary "topk" 50))
    (setq result_topk (torch_hf_generate model_path prompt_tokens eos_id 100 topk_options))
    
    ; 4. Nucleus/Top-P sampling (diversit√© dynamique)
    (setq topp_options (dictionary "topp" 0.9))
    (setq result_topp (torch_hf_generate model_path prompt_tokens eos_id 100 topp_options))
    
    ; 5. G√©n√©ration avec callback pour monitoring en temps r√©el
    ; EXEMPLE CONCRET: Fonction display pour afficher chaque token g√©n√©r√©
    (defun display_token(token_id tokenizer)
        (printerr (tokenizer (decode (integers token_id))))
    )
    
    (setq callback_options (dictionary 
        "topk" 30                      ; Top-K sampling avec K=30
        "callback" 'display_token      ; R√©f√©rence √† la fonction (avec quote)
        "data" tokenizer               ; Passer le tokenizer comme donn√©es
    ))
    (setq result_callback (torch_hf_generate model_path prompt_tokens eos_id 100 callback_options))
    
    ; Alternative avec lambda inline
    (setq lambda_options (dictionary
        "topp" 0.95
        "callback" (lambda (token_id data)
            (println "Token g√©n√©r√©:" token_id)
            (printerr (data (decode (integers token_id))))  ; data = tokenizer
        )
        "data" tokenizer
    ))
    
    result_callback
)
```

## Tokenisation

### Tokenizer TikToken

```lisp
; Classe Tokenizer utilisant TikToken
(class@ Tokenizer (tokenizer_path init)
    (defun configure()
        ; Charger la configuration
        (setq config (json_parse (fread (+ tokenizer_path "/tokenizer_config.json"))))
        (setq vocab (json_parse (fread (+ tokenizer_path "/tokenizer.json"))))
        
        ; Cr√©er le tokenizer
        (setqi tokenizer_obj (tiktoken_create
            (@ vocab "model" "vocab")
            (@ vocab "added_tokens") 
            (@ vocab "pre_tokenizer" "pretokenizers" 0 "pattern" "Regex")
        ))
        
        ; R√©cup√©rer les tokens sp√©ciaux
        (setqi bos_id (tiktoken_special_encode tokenizer_obj "<|begin_of_text|>"))
        (setqi eos_id (tiktoken_special_encode tokenizer_obj "<|end_of_text|>"))
    )
    
    (defun encode(text)
        ; Encoder le texte avec tokens sp√©ciaux
        (setq tokens (tiktoken_encode tokenizer_obj text))
        (pushfirst tokens bos_id)
        tokens
    )
    
    (defun decode(token_list)
        ; D√©coder les tokens
        (tiktoken_decode tokenizer_obj token_list)
    )
    
    (defun eos_id()
        eos_id
    )
)
```

### Utilisation du Tokenizer

```lisp
; Cr√©er et configurer le tokenizer
(setq tok (Tokenizer "/path/to/tokenizer"))
(withclass Tokenizer
    (tok (configure))
)

; Encoder du texte
(setq tokens (tok (encode "Bonjour, comment allez-vous ?")))
(println "Tokens:" tokens)

; D√©coder les tokens
(setq text (tok (decode tokens)))
(println "Texte d√©cod√©:" text)
```

## Flash Attention

### Attention M√©moire-Efficace

```lisp
; Cr√©er les tenseurs d'attention pour longues s√©quences
(setq batch_size 2)
(setq num_heads 8) 
(setq seq_length 4096)  ; S√©quence longue
(setq head_dim 64)

; Tenseurs Query, Key, Value
(setq query (tensor_randn (integers batch_size num_heads seq_length head_dim)))
(setq key (tensor_randn (integers batch_size num_heads seq_length head_dim)))
(setq value (tensor_randn (integers batch_size num_heads seq_length head_dim)))

; Flash Attention - O(N) en m√©moire au lieu de O(N¬≤)
(setq scale (/ 1.0 (sqrt head_dim)))
(setq attention_output (torch_flash_attention query key value scale))

(println "‚úì Flash Attention calcul√©e efficacement")
(println "Forme de sortie:" (tensor_shape attention_output))
```

### Flash Attention avec Masque

```lisp
; Cr√©er un masque causal (utiliser triu pour triangulaire sup√©rieur)
(setq causal_mask (tensor_triu (tensor_ones (integers seq_length seq_length)) 1))

; Cr√©er module Flash Attention et appliquer avec masque
(setq flash_attention (torch_flash_attention_create (* num_heads head_dim) num_heads 0.0 false))
(setq masked_output (torch_flash_attention_with_mask 
    flash_attention query key value causal_mask))
```

## Optimisation et Quantification

### Quantification FP16

```lisp
; Quantification d'un mod√®le en FP16 (50% de r√©duction m√©moire)
(setq model_weights (tensor_randn (integers 512 768)))
(setq fp16_weights (torch_quantize_fp16 model_weights))

; V√©rification de la taille
(setq original_size (tensor_size model_weights))
(setq compressed_size (tensor_size fp16_weights))
(println "Compression FP16 - Ratio:" (/ compressed_size original_size))
```

### Quantification INT8

```lisp
; Quantification INT8 (75% de r√©duction m√©moire)
(setq int8_weights (torch_quantize_int8 model_weights))

; D√©quantification pour v√©rification
(setq reconstructed (torch_dequantize int8_weights))
(setq mse_error (torch_mse_loss model_weights reconstructed))
(println "Erreur de reconstruction INT8:" (tensor_item mse_error))
```

### Quantification Dynamique de Mod√®le

```lisp
; Quantification compl√®te d'un mod√®le
(setq quantized_model (torch_model_quantize_dynamic model "qint8"))
(println "‚úì Mod√®le quantifi√© pour d√©ploiement production")

; Comparaison des performances (exemple conceptuel)
(println "‚úì Mod√®le quantifi√© pour d√©ploiement production")
(println "Note: La quantification r√©duit l'usage m√©moire et peut acc√©l√©rer l'inf√©rence")
```

## Tutoriel Complet d'Entra√Ænement LoRA

Cette section fournit un guide complet pour impl√©menter le fine-tuning LoRA (Low-Rank Adaptation) avec LispE PyTorch, bas√© sur un exemple concret avec le mod√®le Llama 3.1-8B.

### Vue d'ensemble

LoRA (Low-Rank Adaptation) est une technique r√©volutionnaire de fine-tuning qui r√©sout l'un des d√©fis majeurs de l'adaptation des grands mod√®les de langage : la consommation excessive de ressources computationnelles et m√©moire.

#### Le Probl√®me du Fine-tuning Traditionnel

Le fine-tuning classique n√©cessite de mettre √† jour tous les param√®tres d'un mod√®le, ce qui pour un mod√®le comme Llama 3.1-8B repr√©sente :
- **8 milliards de param√®tres** √† entra√Æner
- **32+ GB de m√©moire** pour les gradients seuls
- **Heures ou jours d'entra√Ænement** sur GPU haut de gamme
- **Risque de catastrophic forgetting** des connaissances pr√©-apprises

#### La Solution LoRA

LoRA se base sur l'hypoth√®se que les mises √† jour lors du fine-tuning ont un "rang intrins√®que faible". Au lieu de modifier directement les poids W d'une couche, LoRA introduit deux petites matrices A et B telles que :

```
W' = W + Œ±/r √ó A √ó B
```

O√π :
- **W** : matrice de poids originale (fig√©e)
- **A** : matrice de rang faible (r √ó d)
- **B** : matrice de rang faible (d √ó r) 
- **Œ±** : facteur d'√©chelle
- **r** : rang de d√©composition (typiquement 8-64)

#### Avantages Techniques

1. **R√©duction Drastique des Param√®tres** :
   - Pour une couche 4096√ó4096 avec r=16 : 16M ‚Üí 131K param√®tres (99% de r√©duction)
   - M√©moire GPU divis√©e par 10-100x

2. **Pr√©servation des Connaissances** :
   - Les poids pr√©-entra√Æn√©s restent intacts
   - Pas de catastrophic forgetting
   - Possibilit√© de combiner plusieurs adaptateurs

3. **Flexibilit√© de D√©ploiement** :
   - Adaptateurs de quelques MB vs mod√®les de GB
   - Changement d'adaptateur √† la vol√©e
   - Stockage et distribution simplifi√©s

4. **Performance Maintenue** :
   - R√©sultats comparables au fine-tuning complet
   - Convergence souvent plus rapide
   - Moins de sur-apprentissage

#### Applications Pratiques

- **Adaptation domaine-sp√©cifique** : m√©dical, juridique, technique
- **Personnalisation** : style d'√©criture, ton, format de r√©ponse
- **Multi-t√¢ches** : plusieurs adaptateurs pour diff√©rentes t√¢ches
- **Prototypage rapide** : tests d'hypoth√®ses avec ressources limit√©es

Cette impl√©mentation LispE PyTorch offre une interface simple et puissante pour exploiter LoRA avec une gestion m√©moire optimis√©e, particuli√®rement adapt√©e aux environnements avec contraintes de ressources.

### Structure du Projet

```
projet_entrainement/
‚îú‚îÄ‚îÄ llama3.1-8B/
‚îÇ   ‚îú‚îÄ‚îÄ model/                 # Fichiers du mod√®le (pytorch_model.bin, config.json, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ tokenizer/            # Fichiers du tokenizer (vocab, special_tokens_map.json, etc.)
‚îú‚îÄ‚îÄ tamgu_dataset.json        # Dataset d'entra√Ænement au format chat
‚îú‚îÄ‚îÄ tamgu_lora_adapters_v2/   # R√©pertoire de sortie pour les adaptateurs LoRA
‚îú‚îÄ‚îÄ checkpoints_v2/           # Checkpoints d'entra√Ænement
‚îî‚îÄ‚îÄ lora_training.lisp        # Script d'entra√Ænement principal
```

### Configuration Initiale

```lisp
; Charger les librairies requises
(use 'lispe_torch)
(use 'lispe_tiktoken)

; Configuration globale
(setq model-path (+ _current "llama3.1-8B/model"))
(setq tiktoken-path (+ _current "llama3.1-8B/tokenizer"))
(setq dataset-path (+ _current "tamgu_dataset.json"))
(setq output-dir (+ _current "tamgu_lora_adapters_v2"))
(setq checkpoint-dir (+ _current "checkpoints_v2"))

; Configuration LoRA - POINT CL√â: Param√®tres de la d√©composition W' = W + Œ±/r √ó A √ó B
(setq lora-config (dictionary
    "rank" 16                                          ; üìä RANG r=16: d√©termine la taille des matrices A(r√ód) et B(d√ór)
                                                        ; Plus petit = moins de param√®tres mais capacit√© r√©duite
    "alpha" 32                                         ; üìà FACTEUR Œ±=32: contr√¥le l'amplitude des adaptations LoRA
                                                        ; Ratio Œ±/r = 32/16 = 2.0 (scaling des corrections)
    "target_modules" (strings "q_proj" "k_proj" "v_proj" "o_proj")  ; üéØ MODULES CIBL√âS: seulement les projections d'attention
                                                                      ; √âvite q_proj et k_proj pour pr√©server l'alignement s√©mantique
))

; Configuration d'entra√Ænement - OPTIMIS√âE pour LoRA
(setq training-config (dictionary
    "learning_rate" 2e-4                               ; üéØ LR √âLEV√â: LoRA peut supporter des taux plus √©lev√©s (vs 5e-5 classique)
                                                        ; car les matrices A,B sont initialis√©es √† z√©ro ‚Üí pas de perturbation initiale
    "weight_decay" 0.01                                ; üõ°Ô∏è R√âGULARISATION FAIBLE: √©vite de contraindre les petites matrices LoRA
    "num_epochs" 3                                     ; ‚ö° CONVERGENCE RAPIDE: LoRA converge plus vite que le fine-tuning complet
    "batch_size" 1                                     ; üíæ BATCH R√âDUIT: compens√© par l'accumulation pour √©conomiser la m√©moire
    "gradient_accumulation_steps" 4                    ; üîÑ ACCUMULATION: simule batch_size=4 sans surcharge m√©moire
                                                        ; Essentiel avec les contraintes m√©moire de LoRA
    "max_seq_length" 256                               ; üìè S√âQUENCES COURTES: permet plus d'√©chantillons avec m√©moire limit√©e
    "logging_steps" 10
    "save_steps" 100                                   ; üíæ CHECKPOINTS FR√âQUENTS: adaptateurs LoRA sont l√©gers (quelques MB)
    "eval_steps" 50
    "warmup_steps" 100                                 ; üî• WARMUP IMPORTANT: stabilise l'entra√Ænement des petites matrices
    "max_grad_norm" 1.0                                ; ‚úÇÔ∏è GRADIENT CLIPPING: pr√©vient l'instabilit√© des matrices de rang faible
    "scheduler_type" "linear_warmup_cosine"
    "min_lr" 1e-6
    "device" "mps"  ; "mps" pour Apple Silicon, "cuda" pour NVIDIA, "cpu" sinon
))
```

### Impl√©mentation du Tokenizer TikToken

```lisp
; Classe tokenizer TikToken pour les mod√®les Llama
(class@ TiktokenTokenizer (tokenizer_path init)
    (defun configure()
        (printerrln "üìù Configuration du tokenizer tiktoken...")
        
        ; Charger les fichiers de configuration du tokenizer
        (setq spec_tokens (json_parse (fread (+ tokenizer_path "/special_tokens_map.json"))))
        (setq tok_file (json_parse (fread (+ tokenizer_path "/tokenizer.json"))))
        
        ; Extraire les tokens sp√©ciaux
        (setqi bos_token (@ spec_tokens "bos_token" "content"))
        (setqi eos_token (@ spec_tokens "eos_token" "content"))
        (setq pattern (@ tok_file "pre_tokenizer" "pretokenizers" 0 "pattern" "Regex"))

        ; Cr√©er l'objet tokenizer
        (setqi tokenizer_obj (tiktoken_create
            (@ tok_file "model" "vocab")
            (@ tok_file "added_tokens")
            pattern))

        ; Obtenir les IDs des tokens sp√©ciaux
        (setqi bos_id (tiktoken_special_encode tokenizer_obj bos_token))
        (setqi eos_id (tiktoken_special_encode tokenizer_obj eos_token))
        (setqi pad_id 0)

        (printerrln "‚úì Tokenizer configur√© - taille vocab:" (tiktoken_vocab_size tokenizer_obj))
    )

    ; Formater le texte au format template de chat
    (defun encode_chat_format(instruction response)
        (setq formatted_text (+
            "<|begin_of_text|>"
            "<|start_header_id|>system<|end_header_id|>\n\n"
            "Vous √™tes un assistant utile qui conna√Æt bien les langages de programmation."
            "<|eot_id|>"
            "<|start_header_id|>user<|end_header_id|>\n\n"
            instruction
            "<|eot_id|>"
            "<|start_header_id|>assistant<|end_header_id|>\n\n"
            response
            "<|eot_id|>"
        ))

        ; Encoder et ajouter le token BOS
        (setq tokens (tiktoken_encode tokenizer_obj formatted_text))
        (pushfirst tokens bos_id)

        ; Tronquer si trop long
        (if (> (size tokens) (@ training-config "max_seq_length"))
            (setq tokens (@@ tokens 0 (@ training-config "max_seq_length")))
        )

        tokens
    )

    ; D√©coder les tokens en texte
    (defun decode(token_ids)
        (tiktoken_decode tokenizer_obj token_ids)
    )
)
```

### Gestion du Dataset

```lisp
; Classe gestionnaire de dataset pour les donn√©es d'entra√Ænement
(class@ DatasetManager (dataset_path tiktokenizer init)
    (defun configure()
        (printerrln "üìö Chargement du dataset...")
        
        ; Charger et parser le dataset JSON
        (setq raw_data (json_parse (fread dataset_path)))
        (setqi samples (list))
        (setqi validation_samples (list))

        ; Diviser en train/validation (80/20)
        (setq total_size (size raw_data))
        (setq train_size (floor (* total_size 0.8)))

        (printerrln "‚öôÔ∏è  Pr√©paration des √©chantillons...")
        (loopcount total_size i
            (setq sample (@ raw_data i))
            (setq instruction (@ sample "instruction"))
            (setq response (@ sample "response"))

            ; Tokeniser en utilisant le format chat
            (setq tokens (tiktokenizer TiktokenTokenizer 
                (encode_chat_format instruction response)))
            (setq token_tensor (tensor_create tokens))

            ; Ajouter au dataset appropri√©
            (if (< i train_size)
                (push samples token_tensor)
                (push validation_samples token_tensor)
            )

            ; Indicateur de progression
            (if (== (% i 100) 0)
                (printerr ".")
            )
        )

        (printerrln "\n‚úì Dataset pr√©par√©:")
        (printerrln "  ‚Ä¢ Entra√Ænement:" (size samples) "√©chantillons")
        (printerrln "  ‚Ä¢ Validation:" (size validation_samples) "√©chantillons")
    )

    ; Obtenir un batch d'entra√Ænement
    (defun get_batch(start_idx batch_size)
        (setq batch (list))
        (setq end_idx (min (+ start_idx batch_size) (size samples)))

        (loopcount (- end_idx start_idx) i
            (push batch (@ samples (+ start_idx i)))
        )
        batch
    )

    ; Obtenir un batch de validation
    (defun get_validation_batch(max_samples)
        (setq val_batch (list))
        (setq num_samples (min max_samples (size validation_samples)))

        (loopcount num_samples i
            (push val_batch (@ validation_samples i))
        )
        val_batch
    )
)
```

### Chargement du Mod√®le et Configuration LoRA

```lisp
; Classe principale d'entra√Ænement LoRA
(class@ LoRATrainerV2 (model_path tokenizer_path dataset_path config)
    (defun configure()
        (printerrln "‚öôÔ∏è  Configuration du LoRA Trainer V2...")
        (setqi model_name "llama31_lora")
        (setqi optimizer nil)
        (setqi scheduler nil)
        (setqi global_step 0)
        (setqi best_loss 1000.0)
        true
    )

    ; Charger le mod√®le avec support LoRA
    (defun load_model()
        (printerrln "\nüöÄ Chargement du mod√®le avec LoRA...")

        ; Charger le mod√®le HuggingFace avec infrastructure LoRA
        (torch_hf_load_model_lora
            model_name
            model_path
            (dictionary "device" (@ config "device")))

        ; Afficher les informations du mod√®le
        (setq memory_usage (torch_hf_memory_usage model_name))
        (setq memory_gb (/ memory_usage 1073741824.0))

        (printerrln "‚úì Mod√®le HuggingFace charg√© avec support LoRA:")
        (printerrln "  ‚Ä¢ Nom:" model_name)
        (printerrln "  ‚Ä¢ Chemin:" model_path)
        (printerrln "  ‚Ä¢ Device:" (@ config "device"))
        (printerrln "  ‚Ä¢ M√©moire:" memory_gb "GB")

        true
    )

    ; Initialiser les adaptateurs LoRA
    (defun setup_lora()
        (printerrln "\nüé® Initialisation des adaptateurs LoRA...")

            ; üîß INITIALISATION CRITIQUE des matrices LoRA A et B
        ; POINT CL√â: Impl√©mente la d√©composition W' = W + Œ±/r √ó A √ó B
        (torch_hf_lora_init
            model_name
            (@ lora-config "rank")                      ; r=16: taille des matrices A(16√ó4096) et B(4096√ó16)
            (@ lora-config "alpha")                     ; Œ±=32: facteur d'√©chelle pour contr√¥ler l'amplitude
            (@ lora-config "target_modules")            ; Seulement q_proj, k_proj, v_proj, o_proj de l'attention
            "bfloat16")                                 ; üéØ PR√âCISION: m√™me dtype que le mod√®le base pour coh√©rence

        (printerrln "‚úì Adaptateurs LoRA initialis√©s:")
        (printerrln "  ‚Ä¢ Rang:" (@ lora-config "rank"))
        (printerrln "  ‚Ä¢ Alpha:" (@ lora-config "alpha"))
        (printerrln "  ‚Ä¢ Modules:" (@ lora-config "target_modules"))

        ; üìä R√âCUP√âRATION des param√®tres LoRA UNIQUEMENT (matrices A et B)
        ; AVANTAGE CL√â: Seules ces matrices sont entra√Ænables, pas les poids W originaux
        (setq lora_params (torch_hf_lora_get_parameters model_name))
        (printerrln "  ‚Ä¢ Param√®tres LoRA:" (size lora_params) "tenseurs")
        
        ; üí° CALCUL DE LA R√âDUCTION: pour une couche 4096√ó4096 avec r=16
        ; Param√®tres originaux: 4096√ó4096 = 16M
        ; Param√®tres LoRA: 2√ó(4096√ó16) = 131K ‚Üí r√©duction de 99.2%

        lora_params
    )

    ; √âtape d'entra√Ænement avec accumulation de gradients
    (defun train_step(input_tensor is_accumulating)
        ; Remettre les gradients √† z√©ro au d√©but du cycle d'accumulation
        (check (not is_accumulating)
            (torch_optimizer_zero_grad optimizer)
        )

        ; üöÄ PROPAGATION AVANT avec LoRA int√©gr√© automatiquement
        ; POINT CL√â: torch_hf_forward applique W' = W + Œ±/r √ó A √ó B de fa√ßon transparente
        (setq input_2d (tensor_reshape input_tensor (integers 1 -1)))
        (setq output (torch_hf_forward model_name input_2d))           ; Les corrections LoRA sont ajout√©es automatiquement
                                                                       ; aux projections d'attention q, k, v, o

        ; Calculer la loss de mod√©lisation du langage
        (setq loss (calculate_language_modeling_loss output input_2d))

        (check loss
            ; Normaliser la loss par les √©tapes d'accumulation
            (setq accum_steps (@ config "gradient_accumulation_steps"))
            (setq scaled_loss (tensor_div loss (tensor_create (floats accum_steps))))
            
            ; Propagation arri√®re
            (torch_backward scaled_loss)

            ; Synchronisation m√©moire pour MPS
            (torch_mps_synchronize)

            loss
        )
    )

    ; Boucle d'entra√Ænement principale
    (defun train()
        (printerrln "\nüöÄ D√©but de l'entra√Ænement LoRA...")

        (setq num_epochs (@ config "num_epochs"))
        (setq batch_size (@ config "batch_size"))

        (loopcount num_epochs epoch
            (printerrln "\nüìñ √âpoque" (+ epoch 1) "/" num_epochs)

            ; Impl√©mentation de la boucle d'entra√Ænement
            ; ... (traitement des batches, calcul de loss, validation)

            (printerrln "‚úì √âpoque" (+ epoch 1) "termin√©e")
        )

        (printerrln "üéâ Entra√Ænement termin√© avec succ√®s !")
        true
    )
)
```

### Calcul de la Loss

```lisp
; Calculateur de loss pour mod√©lisation du langage
(class@ LossCalculator (init)
    (defun calculate_language_modeling_loss(logits input_tokens)
        (setq logits_shape (tensor_shape logits))
        (setq batch_size (@ logits_shape 0))
        (setq seq_len (@ logits_shape 1))
        (setq vocab_size (@ logits_shape 2))

        (check (> seq_len 1)
            ; Pr√©dictions : logits pour le token suivant √† chaque position
            (setq pred_logits (tensor_slice logits 1 0 (- seq_len 1)))
            
            ; Cibles : tokens suivants r√©els (d√©cal√©s de 1)
            (setq target_tokens (tensor_slice input_tokens 1 1 seq_len))

            ; Redimensionner pour la loss d'entropie crois√©e
            (setq pred_flat (tensor_reshape pred_logits 
                (integers (* batch_size (- seq_len 1)) vocab_size)))
            (setq target_flat (tensor_reshape target_tokens 
                (integers (* batch_size (- seq_len 1)))))

            ; Calculer la loss d'entropie crois√©e
            (setq loss (torch_cross_entropy pred_flat target_flat))

            ; Synchroniser pour la gestion m√©moire
            (torch_mps_synchronize)

            loss
        )
    )
)
```

### Ex√©cution de l'Entra√Ænement

```lisp
; Ex√©cution principale
(printerrln "üöÄ Initialisation du fine-tuning LoRA...\n")

; Cr√©er l'instance de trainer
(setq trainer (LoRATrainerV2
    model-path
    tiktoken-path
    dataset-path
    training-config))

; Ex√©cuter le pipeline d'entra√Ænement
(withclass LoRATrainerV2
    (trainer (configure))
    (if (trainer (load_model))
        (if (trainer (setup_components))
            (if (trainer (train))
                (printerrln "\n‚úÖ Fine-tuning LoRA termin√© avec succ√®s ! üéâ")
                (printerrln "\n‚ùå √âchec de l'entra√Ænement")
            )
            (printerrln "\n‚ùå √âchec de la configuration des composants")
        )
        (printerrln "\n‚ùå √âchec du chargement du mod√®le")
    )
)
```

### Fonctionnalit√©s Cl√©s

1. **Efficacit√© M√©moire** : Utilise l'accumulation de gradients et la synchronisation MPS pour une utilisation m√©moire optimale
2. **Design Modulaire** : Classes s√©par√©es pour tokenizer, dataset, calcul de loss et entra√Ænement
3. **Monitoring** : Journalisation compl√®te de la loss, du taux d'apprentissage et de l'utilisation m√©moire
4. **Checkpointing** : Sauvegarde r√©guli√®re des adaptateurs LoRA et de l'√©tat d'entra√Ænement
5. **Validation** : √âvaluation p√©riodique sur des donn√©es de test
6. **Support Multi-Device** : D√©tection automatique du meilleur device disponible (CUDA/MPS/CPU)

### Format du Dataset

Le dataset d'entra√Ænement doit √™tre au format JSON avec des paires instruction-r√©ponse :

```json
[
    {
        "instruction": "Comment d√©finir une fonction en Tamgu ?",
        "response": "En Tamgu, vous d√©finissez une fonction en utilisant le mot-cl√© 'function'..."
    },
    {
        "instruction": "Quels sont les types de donn√©es de base en Tamgu ?",
        "response": "Tamgu supporte plusieurs types de donn√©es incluant..."
    }
]
```

Cet exemple complet d√©montre comment impl√©menter un fine-tuning LoRA efficace avec une gestion m√©moire appropri√©e, un monitoring complet et une organisation modulaire du code en utilisant LispE PyTorch.

## Exemples Pratiques

### Exemple 1 : Classification Simple

```lisp
; Entra√Æner un classificateur simple avec tenseurs PyTorch
(defun train_simple_classifier()
    ; Cr√©er des tenseurs d'exemple
    (setq train_data (tensor_randn (integers 100 784)))  ; 100 exemples
    (setq train_labels (tensor_randint 0 10 (integers 100)))  ; 10 classes
    
    ; Cr√©er couches
    (setq linear1 (torch_linear 784 128))
    (setq linear2 (torch_linear 128 10))
    
    ; Cr√©er optimiseur
    (setq optimizer (torch_adamw_optimizer 0.001))
    (torch_optimizer_add_params optimizer linear1)
    (torch_optimizer_add_params optimizer linear2)
    
    ; Boucle d'entra√Ænement
    (loop epoch 50
        ; Forward pass
        (setq h1 (tensor_relu (torch_linear_forward linear1 train_data)))
        (setq predictions (torch_linear_forward linear2 h1))
        (setq loss (torch_crossentropy_loss predictions train_labels))
        
        (torch_optimizer_zero_grad optimizer)
        (torch_backward loss)
        (torch_optimizer_step optimizer)
        
        (if (== (% epoch 10) 0)
            (println "Epoch" epoch "Loss:" (tensor_item loss)))
    )
    
    model
)
```

### Exemple 2 : Fine-tuning d'un Mod√®le de Langage

```lisp
; Fine-tuning complet avec LoRA
(defun finetune_language_model(model_path dataset_path)
    ; Configuration
    (setq config (dictionary
        "device" "mps"
        "dtype" "float16"
        "low_memory" true
    ))
    
    ; Charger mod√®le et tokenizer
    (setq model_name "llama_ft")
    (torch_hf_load_model_lora model_path config model_name)
    
    (setq tokenizer (Tokenizer (+ model_path "/tokenizer")))
    (withclass Tokenizer (tokenizer (configure)))
    
    ; Initialiser LoRA
    (setq lora_config (dictionary
        "rank" 16
        "alpha" 32
        "target_modules" (strings "q_proj" "k_proj" "v_proj" "o_proj")
    ))
    (torch_hf_lora_init model_name lora_config)
    
    ; Charger dataset
    (setq dataset (json_parse (fread dataset_path)))
    
    ; Configuration d'entra√Ænement
    (setq train_config (dictionary
        "learning_rate" 2e-4
        "num_epochs" 3
        "batch_size" 1
        "max_seq_length" 512
    ))
    
    ; Cr√©er optimiseur
    (setq lora_params (torch_hf_lora_get_parameters model_name))
    (setq learning_rate (@ train_config "learning_rate"))
    (setq optimizer (torch_adamw_optimizer learning_rate))
    (torch_optimizer_add_params optimizer lora_params)
    
    ; Boucle d'entra√Ænement
    (loop epoch (@ train_config "num_epochs")
        (println "=== Epoch" (+ epoch 1) "===")
        
        (loop sample_idx (size dataset)
            (setq sample (@ dataset sample_idx))
            (setq instruction (@ sample "instruction"))
            (setq response (@ sample "output"))
            
            ; Pr√©parer les tokens
            (setq formatted_text (+
                "<|start_header_id|>user<|end_header_id|>\n"
                instruction
                "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n"
                response
                "<|eot_id|>"
            ))
            
            (setq tokens (tokenizer (encode formatted_text)))
            (setq input_tensor (tensor_unsqueeze (tensor_create tokens) 0))
            
            ; Forward pass
            (setq logits (torch_hf_forward model_name input_tensor))
            
            ; Calcul de la loss (language modeling)
            (setq loss (calculate_language_modeling_loss logits input_tensor))
            
            ; Backward pass
            (torch_optimizer_zero_grad optimizer)
            (torch_backward loss)
            (torch_optimizer_step optimizer)
            
            ; Logging
            (if (== (% sample_idx 50) 0)
                (println "  Sample" sample_idx "/" (size dataset) 
                        "Loss:" (tensor_item loss)))
        )
        
        ; Sauvegarder les adaptateurs LoRA
        (setq checkpoint_path (+ "./checkpoints/epoch_" epoch))
        (torch_hf_lora_save model_name checkpoint_path)
        (println "‚úì Checkpoint sauvegard√©:" checkpoint_path)
    )
    
    model_name
)
```

### Exemple 3 : G√©n√©ration Interactive

```lisp
; Syst√®me de g√©n√©ration interactive
(defun interactive_generation(model_path tokenizer_path)
    ; Charger le mod√®le
    (setq config (dictionary "device" "mps"))
    (setq model_id (torch_hf_load_model model_path config))
    
    ; Charger le tokenizer
    (setq tok (Tokenizer tokenizer_path))
    (withclass Tokenizer (tok (configure)))
    
    ; Boucle interactive
    (loop true
        (print "Prompt (ou 'quit' pour sortir): ")
        (setq user_input (input))
        
        (check (== user_input "quit")
            (break)
        )
        
        ; G√©n√©rer la r√©ponse
        (println "\nG√©n√©ration en cours...")
        (setq response (generate_response model_id tok user_input))
        (println "R√©ponse g√©n√©r√©e.")
        (println)
    )
)

(defun generate_response(model_id tokenizer prompt)
    ; Encoder le prompt
    (setq prompt_tokens (tokenizer (encode prompt)))
    (setq context_id (torch_hf_enable_kv_cache model_id true))
    
    ; G√©n√©rer la r√©ponse
    (setq current_input (tensor_unsqueeze (tensor_create prompt_tokens) 0))
    (setq generated (clone prompt_tokens))
    
    (print "R√©ponse: ")
    (loop i 150  ; max_tokens
        (setq logits (torch_hf_forward model_id current_input context_id))
        (setq last_logits (tensor_select logits 1 -1))
        (setq last_logits (tensor_select last_logits 0 0))
        
        ; Sampling avec temp√©rature
        (setq scaled_logits (tensor_div_scalar last_logits 0.7))
        (setq probs (tensor_softmax scaled_logits -1))
        (setq next_token (tensor_multinomial probs 1 true))
        (setq next_token_id (tensor_item next_token))
        
        ; Afficher le token
        (setq token_text (tokenizer (decode (integers next_token_id))))
        (print token_text)
        
        ; V√©rifier fin de g√©n√©ration
        (check (== next_token_id (tokenizer (eos_id)))
            (break)
        )
        
        (push generated next_token_id)
        (setq current_input (tensor_unsqueeze (tensor_create (integers next_token_id)) 0))
    )
    
    (println)
    generated
)
```

## Bonnes Pratiques et Optimisations

### Gestion M√©moire

```lisp
; Utiliser des tenseurs optimis√©s LispE
(setq data (floats 1.0 2.0 3.0))  ; Plus efficace que (list 1.0 2.0 3.0)

; Lib√©rer explicitement la m√©moire GPU si n√©cessaire
(torch_cuda_empty_cache)  ; CUDA
```

### Performance

```lisp
; Utiliser le bon device pour vos tenseurs
(setq tensor (tensor_randn (integers 100 100)))
; D√©placer vers le device appropri√© selon le besoin
(if (equal device "cuda")
    (setq tensor (torch_to_cuda tensor))
    (if (equal device "mps")
        (setq tensor (torch_to_mps tensor))))

; Grouper les op√©rations pour √©viter les synchronisations
(setq result (tensor_add 
    (tensor_mul a b) 
    (tensor_div c d)))
```

### D√©bogage

```lisp
; V√©rifier les formes des tenseurs
(defun debug_tensor(tensor name)
    (println name "- Forme:" (tensor_shape tensor))
)

; Surveiller l'utilisation m√©moire
(defun memory_info()
    (if (torch_cuda_is_available)
        (println "M√©moire CUDA utilis√©e:" (torch_cuda_memory_allocated) "bytes")
        (println "Utilisation CPU"))
)
```

# Fonctions Disponibles

Cette section liste toutes les fonctions disponibles dans la librairie LispE PyTorch, organis√©es par famille et par ordre alphab√©tique.

## Fonctions Attention

### `torch_attention_forward(attention query key value)`
Applique l'attention multi-t√™tes aux tenseurs query, key et value avec un module d'attention pr√©-cr√©√©.

### `torch_attention_mask(sequences pad_token)`
Cr√©e un masque d'attention pour masquer les tokens de padding dans les s√©quences.

### `torch_flash_attention(query key value)`
Applique l'algorithme Flash Attention optimis√© pour la m√©moire aux tenseurs Q, K, V.

### `torch_flash_attention_create(embed_dim num_heads dropout bias)`
Cr√©e un module Flash Attention avec la dimension d'embedding, le nombre de t√™tes, le taux de dropout et l'option bias.

### `torch_flash_attention_forward(flash_attention query key value)`
Effectue la propagation avant du module Flash Attention avec les tenseurs Q, K, V.

### `torch_flash_attention_with_dropout(flash_attention query key value dropout_p training)`
Applique Flash Attention avec un taux de dropout personnalis√© en mode entra√Ænement ou √©valuation.

### `torch_flash_attention_with_mask(flash_attention query key value attn_mask)`
Applique Flash Attention avec un masque d'attention pour masquer certaines positions.

### `torch_(embed_dim num_heads)`
Cr√©e un module d'attention multi-t√™tes avec la dimension d'embedding et le nombre de t√™tes sp√©cifi√©s.

### `torch_scaled_dot_product_attention(query key value attn_mask dropout_p is_causal scale)`
Impl√©mente l'attention scaled dot-product avec masque, dropout, option causale et facteur d'√©chelle.

## Fonctions Device et GPU

### `torch_cuda_device_count()`
Retourne le nombre de dispositifs CUDA disponibles sur le syst√®me.

### `torch_cuda_empty_cache()`
Vide le cache m√©moire CUDA pour lib√©rer la m√©moire GPU non utilis√©e.

### `torch_cuda_is_available()`
V√©rifie si CUDA est disponible sur le syst√®me.

### `torch_cuda_memory_allocated()`
Retourne la quantit√© de m√©moire CUDA actuellement allou√©e en octets.

### `torch_cuda_memory_total()`
Retourne la quantit√© totale de m√©moire CUDA disponible en octets.

### `torch_get_best_device()`
D√©termine automatiquement le meilleur device disponible (CUDA, MPS ou CPU).

### `torch_mps_is_available()`
V√©rifie si Metal Performance Shaders (MPS) est disponible sur Apple Silicon.

### `torch_mps_synchronize((safemode))`
Synchronise les op√©rations MPS avec un mode s√©curis√© optionnel.

### `torch_on_mps(tensor)`
V√©rifie si un tenseur est sur le device MPS.

### `torch_set_device(device)`
D√©finit le device par d√©faut pour les op√©rations PyTorch.
Noms du device: "mps", "cuda", "cuda:n" ou "cpu".

### `torch_to_cpu(tensor)`
D√©place un tenseur vers le "cpu"".

### `torch_to_cuda(tensor (device))`
D√©place un tenseur vers un device CUDA sp√©cifique ou par d√©faut.
Noms: "cuda", "cuda:n", o√π n est un index num√©rique d√©finissant un GPU.

### `torch_to_mps(tensor)`
D√©place un tenseur vers le device "mps" (Apple Silicon).

### `torch_to_device(tensor)`
D√©place un tenseur vers un device "mps", "cuda" ou "cpu".

## Fonctions Embedding et Encodage

### `torch_apply_rotary_pos_emb(tensor cos sin)`
Applique l'embedding positionnel rotatif (RoPE) √† un tenseur avec les cosinus et sinus pr√©calcul√©s.

### `torch_embedding(num_embeddings embedding_dim)`
Cr√©e une couche d'embedding avec le nombre d'embeddings et la dimension sp√©cifi√©s.

### `torch_embedding_forward(embedding input_data)`
Effectue la propagation avant d'une couche d'embedding avec les donn√©es d'entr√©e.

### `torch_positional_encoding(d_model max_len)`
Cr√©e un module d'encodage positionnel avec la dimension du mod√®le et la longueur maximale.

### `torch_positional_forward(positional_encoding input_data)`
Applique l'encodage positionnel aux donn√©es d'entr√©e.

### `torch_rotary_embedding(dim max_seq_len)`
Cr√©e un module d'embedding positionnel rotatif (RoPE) avec la dimension et la longueur maximale.

### `torch_rotary_forward(rotary_embedding seq_len device)`
Calcule les cosinus et sinus pour l'embedding rotatif pour une longueur de s√©quence donn√©e.

## Fonctions Flash Attention (voir Fonctions Attention)

## Fonctions G√©n√©ration de Texte

### `torch_generate(generator input_ids strategy)`
G√©n√®re du texte √† partir d'un g√©n√©rateur avec les tokens d'entr√©e et la strat√©gie sp√©cifi√©e.



## Fonctions Gradient Checkpointing

### `torch_checkpoint_create(module)`
Cr√©e un module avec gradient checkpointing pour √©conomiser la m√©moire pendant l'entra√Ænement.

### `torch_checkpoint_disable(module)`
D√©sactive le gradient checkpointing pour un module.

### `torch_checkpoint_enable(module)`
Active le gradient checkpointing pour un module.

### `torch_checkpoint_forward(module input_data)`
Effectue la propagation avant avec gradient checkpointing.

## Fonctions Hugging Face

### `torch_hf_clear_attention_scores(path (kvcache))`
Efface les scores d'attention stock√©s dans le cache.

### `torch_hf_embeddings(path token_ids)`
Obtient les embeddings pour une liste de tokens √† partir d'un mod√®le Hugging Face.

### `torch_hf_enable_kv_cache(path enable)`
Active ou d√©sactive le cache Key-Value pour un mod√®le Hugging Face.

### `torch_hf_forward(path input_ids (kvcache))`
Effectue la propagation avant d'un mod√®le Hugging Face avec cache KV optionnel.

### `torch_hf_forward_attention_scores(path layer_index (kvcache))`
Obtient les scores d'attention pour une couche sp√©cifique du mod√®le.

### `torch_hf_forward_attention_size(path (kvcache))`
Retourne la taille des tenseurs d'attention du mod√®le.

### `torch_hf_forward_manual(path input_ids (kvcache))`
Effectue une propagation avant manuelle avec contr√¥le d√©taill√© du cache.

### `torch_hf_generate(path initial_tokens eos_id max_length (options))`
G√©n√®re du texte √† partir d'un mod√®le Hugging Face avec tokens initiaux et param√®tres.

**Param√®tres :**
- `path` : Chemin/ID du mod√®le charg√©
- `initial_tokens` : Tokens d'entr√©e (liste d'entiers)
- `eos_id` : ID(s) du token de fin (entier ou liste d'entiers)
- `max_length` : Nombre maximum de tokens √† g√©n√©rer
- `options` : Dictionnaire optionnel avec :
  - `"topk"` (integer) : Top-K sampling - consid√®re seulement les K meilleurs tokens
  - `"topp"` (float) : Top-P/Nucleus sampling - probabilit√© cumulative (0.0-1.0)
  - `"greedy"` (boolean) : Sampling glouton - s√©lectionne toujours le token le plus probable
  - `"callback"` (function) : Fonction appel√©e pour chaque token g√©n√©r√©
    - Signature: `(callback token_id data)`
    - `token_id` (integer) : ID du token g√©n√©r√©
    - `data` : Donn√©es pass√©es via le param√®tre "data"
    - Peut √™tre une r√©f√©rence de fonction (avec quote) ou lambda
  - `"data"` (any) : Donn√©es pass√©es au callback (ex: tokenizer, contexte)

**Strat√©gies de sampling (mutuellement exclusives) :**
- **Aucune** : Sampling simple avec softmax + multinomial (d√©faut)
- **`"greedy"`** : D√©terministe, s√©lectionne le token le plus probable
- **`"topk"`** : Limite aux K tokens les plus probables
- **`"topp"`** : Nucleus sampling, limite √† la probabilit√© cumulative P

### `torch_hf_get_down_weight(path layer)`
R√©cup√®re les poids de la projection down d'une couche FFN sp√©cifique.

### `torch_hf_get_gate_up_fused_weight(path layer)`
R√©cup√®re les poids fusionn√©s gate et up d'une couche FFN.

### `torch_hf_get_gate_weight(path layer)`
R√©cup√®re les poids de la projection gate d'une couche FFN sp√©cifique.

### `torch_hf_get_k_weight(path layer)`
R√©cup√®re les poids de la projection Key d'une couche d'attention sp√©cifique.

### `torch_hf_get_o_weight(path layer)`
R√©cup√®re les poids de la projection Output d'une couche d'attention sp√©cifique.

### `torch_hf_get_q_weight(path layer)`
R√©cup√®re les poids de la projection Query d'une couche d'attention sp√©cifique.

### `torch_hf_get_qkv_fused_weight(path layer)`
R√©cup√®re les poids fusionn√©s Query, Key, Value d'une couche d'attention.

### `torch_hf_get_rms_norm_eps(path)`
R√©cup√®re la valeur epsilon utilis√©e pour la normalisation RMS du mod√®le.

### `torch_hf_get_up_weight(path layer)`
R√©cup√®re les poids de la projection up d'une couche FFN sp√©cifique.

### `torch_hf_get_v_weight(path layer)`
R√©cup√®re les poids de la projection Value d'une couche d'attention sp√©cifique.

### `torch_hf_get_weight(path name)`
R√©cup√®re un tenseur de poids sp√©cifique du mod√®le par son nom.

### `torch_hf_list_weights(path)`
Liste tous les noms des tenseurs de poids disponibles dans le mod√®le.

### `torch_hf_load_model(path (config nil))`
Charge un mod√®le Hugging Face depuis un chemin avec configuration optionnelle.

**Param√®tres du dictionnaire config :**
- `"device"` (string) : Device cible ("cuda", "mps", "cpu")
- `"max_seq_len"` (integer) : Longueur maximale de s√©quence (d√©faut: selon config.json)
- `"rope_scaling"` (float) : Facteur d'√©chelle RoPE (d√©faut: 1.0)
- `"temperature"` (float) : Temp√©rature de g√©n√©ration (d√©faut: 1.0)
- `"top_p"` (float) : Nucleus sampling (d√©faut: 1.0)
- `"top_k"` (integer) : Top-K sampling (d√©faut: 0 = d√©sactiv√©)
- `"repetition_penalty"` (float) : P√©nalit√© de r√©p√©tition (d√©faut: 1.0)
- `"use_kv_cache"` (boolean) : Activer cache KV (d√©faut: true)
- `"max_cache_len"` (integer) : Taille max cache (d√©faut: max_seq_len)
- `"manual_attention"` (boolean) : Mode attention manuel (d√©faut: false)

### `torch_hf_lora_enable(model_name enable)`
Active ou d√©sactive les adaptateurs LoRA pour un mod√®le.

### `torch_hf_lora_get_parameters(model_name)`
R√©cup√®re les param√®tres LoRA d'un mod√®le pour l'optimisation.

### `torch_hf_lora_init(model_name rank alpha target_modules (dtype))`
Initialise les adaptateurs LoRA pour un mod√®le avec rang, alpha et modules cibles.

### `torch_hf_lora_load(model_name path)`
Charge des adaptateurs LoRA pr√©-entra√Æn√©s depuis un fichier.

### `torch_hf_lora_merge(model_name)`
Fusionne les adaptateurs LoRA avec les poids principaux du mod√®le.

### `torch_hf_lora_save(model_name path)`
Sauvegarde les adaptateurs LoRA entra√Æn√©s dans un fichier.

### `torch_hf_lora_unmerge(model_name)`
S√©pare les adaptateurs LoRA des poids principaux du mod√®le.

### `torch_hf_memory_usage(path)`
Retourne l'utilisation m√©moire d'un mod√®le Hugging Face charg√©.

### `torch_hf_model_info(path)`
R√©cup√®re des informations d√©taill√©es sur un mod√®le Hugging Face.

### `torch_hf_model_summary(path)`
Affiche un r√©sum√© des caract√©ristiques du mod√®le Hugging Face.

### `torch_hf_reset_kv_cache(kvcache)`
Remet √† z√©ro le cache Key-Value pour repartir d'une s√©quence vide.

## Fonctions JIT (TorchScript)

### `torch_jit_load(model_path (device "cpu"))`
Charge un mod√®le TorchScript depuis un fichier avec device optionnel.

### `torch_jit_model_forward(model tensor)`
Effectue la propagation avant d'un mod√®le JIT avec un tenseur d'entr√©e.

### `torch_jit_model_forward_with_lora(model input)`
Effectue la propagation avant d'un mod√®le JIT avec adaptateurs LoRA.

### `torch_jit_model_get_buffer(model buffername)`
R√©cup√®re un buffer sp√©cifique d'un mod√®le JIT par son nom.

### `torch_jit_model_get_intermediate_states(model input layer_names)`
Obtient les √©tats interm√©diaires de couches sp√©cifiques pendant la propagation.

### `torch_jit_model_get_tensor(model tensorname)`
R√©cup√®re un tenseur sp√©cifique d'un mod√®le JIT par son nom.

### `torch_jit_model_get_tensor_shape(model tensorname)`
Retourne la forme d'un tenseur sp√©cifique dans un mod√®le JIT.

### `torch_jit_model_info(model)`
Affiche des informations d√©taill√©es sur un mod√®le JIT.

### `torch_jit_model_list_buffers(model)`
Liste tous les buffers disponibles dans un mod√®le JIT.

### `torch_jit_model_list_methods(model)`
Liste toutes les m√©thodes disponibles dans un mod√®le JIT.

### `torch_jit_model_list_parameter_names(model)`
Liste tous les noms de param√®tres dans un mod√®le JIT.

### `torch_jit_model_list_tensor_names(model)`
Liste tous les noms de tenseurs disponibles dans un mod√®le JIT.

### `torch_jit_model_register_lora_hook(model layer_name lora_layer)`
Enregistre un hook LoRA pour une couche sp√©cifique d'un mod√®le JIT.

### `torch_jit_model_to_best_device(model)`
D√©place un mod√®le JIT vers le meilleur device disponible.

### `torch_jit_model_to_device(model (device "cpu"))`
D√©place un mod√®le JIT vers un device sp√©cifique.

### `torch_jit_model_to_mps(model)`
D√©place un mod√®le JIT vers le device MPS (Apple Silicon).

### `torch_jit_model_update_weight(model param_name new_weight)`
Met √† jour un poids sp√©cifique d'un mod√®le JIT.

### `torch_jit_unload(model)`
D√©charge un mod√®le JIT de la m√©moire.

## Fonctions Learning Rate Scheduling

### `torch_lr_scheduler(optimizer scheduler_type config)`
Cr√©e un planificateur de taux d'apprentissage avec type et configuration.

### `torch_scheduler_get_lr(scheduler)`
Obtient le taux d'apprentissage actuel d'un planificateur.

### `torch_scheduler_set_lr(scheduler learning_rate)`
D√©finit un nouveau taux d'apprentissage pour un planificateur.

### `torch_scheduler_step(scheduler)`
Effectue une √©tape de planification pour mettre √† jour le taux d'apprentissage.

## Fonctions LoRA

### `torch_hf_load_model_lora(model_name path config)`
Charge un mod√®le Hugging Face avec support LoRA int√©gr√©.

### `torch_lora_apply_to_linear(linear_layer rank alpha)`
Applique des adaptateurs LoRA √† une couche lin√©aire existante.

### `torch_lora_compute_delta(lora_layer)`
Calcule la matrice delta (A√óB) d'une couche LoRA.

### `torch_lora_forward(lora_layer input_data)`
Effectue la propagation avant d'une couche LoRA.

### `torch_lora_forward_with_gradients(lora_layer input retain_graph)`
Effectue la propagation avant LoRA en conservant le graphe de calcul pour les gradients.

### `torch_lora_get_adaptation_magnitude(lora_layer)`
Calcule la magnitude de l'adaptation LoRA par rapport aux poids originaux.

### `torch_lora_linear(in_features out_features rank alpha)`
Cr√©e une couche lin√©aire avec adaptateurs LoRA int√©gr√©s.

### `torch_lora_load_adapters(model path)`
Charge des adaptateurs LoRA sauvegard√©s dans un mod√®le.

### `torch_lora_merge_weights(lora_layer)`
Fusionne les poids LoRA avec les poids de base de la couche.

### `torch_lora_save_adapters(model path)`
Sauvegarde les adaptateurs LoRA d'un mod√®le dans un fichier.

### `torch_lora_trainable_params(model)`
R√©cup√®re uniquement les param√®tres LoRA entra√Ænables d'un mod√®le.

## Fonctions Loss (Perte)

### `torch_backward(loss)`
Effectue la r√©tropropagation √† partir d'un tenseur de perte.

### `torch_cross_entropy(predictions targets)`
Calcule la perte d'entropie crois√©e entre pr√©dictions et cibles.

### `torch_crossentropy_loss(predictions targets)`
Calcule la perte d'entropie crois√©e (alias de torch_cross_entropy).

### `torch_mse_loss(predictions targets)`
Calcule la perte d'erreur quadratique moyenne entre pr√©dictions et cibles.

## Fonctions Mod√®les

### `torch_forward(model input_data)`
Effectue la propagation avant d'un mod√®le avec des donn√©es d'entr√©e.

### `torch_load_checkpoint(path)`
Charge un checkpoint complet contenant mod√®le, optimiseur et √©poque.

### `torch_load_model(model path)`
Charge les poids d'un mod√®le depuis un fichier.

### `torch_load_state_dict(model state_dict)`
Charge un dictionnaire d'√©tat dans un mod√®le.

### `torch_model(input_size hidden_size output_size)`
Cr√©e un mod√®le MLP simple avec tailles d'entr√©e, cach√©e et sortie sp√©cifi√©es.

### `torch_save_checkpoint(model optimizer epoch path)`
Sauvegarde un checkpoint complet avec mod√®le, optimiseur et num√©ro d'√©poque.

### `torch_save_model(model path)`
Sauvegarde les poids d'un mod√®le dans un fichier.

### `torch_state_dict(model)`
R√©cup√®re le dictionnaire d'√©tat d'un mod√®le (tous les param√®tres).

## Fonctions Neural Network

### `torch_linear(in_features out_features)`
Cr√©e une couche lin√©aire (dense) avec nombre de features d'entr√©e et de sortie.

### `torch_layer_norm(normalized_shape)`
Cr√©e une couche de normalisation avec la forme sp√©cifi√©e.

### `torch_layer_norm_forward(layer_norm input_data)`
Applique la normalisation de couche aux donn√©es d'entr√©e.

### `torch_linear_forward(linear input_data)`
Effectue la propagation avant d'une couche lin√©aire.

### `torch_transformer_block(embed_dim num_heads ffn_dim)`
Cr√©e un bloc Transformer avec dimension d'embedding, nombre de t√™tes et dimension FFN.

### `torch_transformer_forward(block input_data)`
Effectue la propagation avant d'un bloc Transformer.

## Fonctions Optimisation

### `torch_adam_optimizer(learning_rate)`
Cr√©e un optimiseur Adam avec le taux d'apprentissage sp√©cifi√©.

### `torch_adamw_optimizer(learning_rate)`
Cr√©e un optimiseur AdamW avec le taux d'apprentissage sp√©cifi√©.

### `torch_clip_grad_norm(optimizer max_norm)`
Applique le clipping de gradient par norme pour √©viter l'explosion des gradients.

### `torch_optimizer(model learning_rate type)`
Cr√©e un optimiseur g√©n√©rique pour un mod√®le avec taux d'apprentissage et type.

### `torch_optimizer_add_params(params learning_rate weight_decay)`
Ajoute des param√®tres √† un optimiseur avec taux d'apprentissage et d√©croissance de poids.

### `torch_optimizer_step(optimizer)`
Effectue une √©tape d'optimisation (mise √† jour des poids).

### `torch_optimizer_zero_grad(optimizer)`
Remet √† z√©ro les gradients de tous les param√®tres de l'optimiseur.

### `torch_set_grad_enabled(enabled)`
Active ou d√©sactive le calcul des gradients globalement.

### `torch_sgd_optimizer(learning_rate)`
Cr√©e un optimiseur SGD (Stochastic Gradient Descent) avec le taux d'apprentissage.

## Fonctions Quantification

### `torch_dequantize(quantized_tensor)`
Dequantifie un tenseur quantifi√© vers sa repr√©sentation en virgule flottante.

### `torch_model_quantize_dynamic(model)`
Applique la quantification dynamique √† un mod√®le complet.

### `torch_model_quantize_static(model calibration_data)`
Applique la quantification statique √† un mod√®le avec donn√©es de calibration.

### `torch_quantize_dynamic(tensor dtype)`
Applique la quantification dynamique √† un tenseur avec le type de donn√©es sp√©cifi√©.

### `torch_quantize_fp16(tensor)`
Quantifie un tenseur en pr√©cision half (16-bit float).

### `torch_quantize_int8(tensor)`
Quantifie un tenseur en entiers 8-bit.

### `torch_quantize_linear(tensor scale zero_point)`
Applique la quantification lin√©aire avec facteur d'√©chelle et point z√©ro.

### `torch_quantize_per_channel(tensor scales zero_points axis)`
Applique la quantification par canal avec facteurs d'√©chelle et points z√©ro.

### `torch_quantize_static(tensor scale zero_point dtype)`
Applique la quantification statique avec param√®tres fixes.

## Fonctions Sampling

### `tensor_multinomial(probs num_samples replacement)`
√âchantillonne √† partir d'une distribution multinomiale avec ou sans remplacement.

### `torch_sort(tensor dim descending)`
Trie un tenseur le long d'une dimension en ordre croissant ou d√©croissant.

### `torch_topk(tensor k dim largest)`
Retourne les k plus grandes (ou plus petites) valeurs le long d'une dimension.

## Fonctions Tenseur - Activation

### `tensor_abs(tensor)`
Calcule la valeur absolue de chaque √©l√©ment du tenseur.

### `tensor_gelu(tensor)`
Applique la fonction d'activation GELU (Gaussian Error Linear Unit).

### `tensor_relu(tensor)`
Applique la fonction d'activation ReLU (Rectified Linear Unit).

### `tensor_sigmoid(tensor)`
Applique la fonction d'activation sigmoid.

### `tensor_silu(tensor)`
Applique la fonction d'activation SiLU (Sigmoid Linear Unit, aussi appel√©e Swish).

### `tensor_softmax(tensor dim)`
Applique la fonction softmax le long de la dimension sp√©cifi√©e.

### `tensor_tanh(tensor)`
Applique la fonction d'activation tangente hyperbolique.

## Fonctions Tenseur - Arithm√©tique

### `tensor_add(tensor1 tensor2)`
Addition √©l√©ment par √©l√©ment de deux tenseurs.

### `tensor_add_scalar(tensor scalar)`
Addition d'un scalaire √† tous les √©l√©ments d'un tenseur.

### `tensor_div(tensor1 tensor2)`
Division √©l√©ment par √©l√©ment de deux tenseurs.

### `tensor_div_scalar(tensor scalar)`
Division de tous les √©l√©ments d'un tenseur par un scalaire.

### `tensor_matmul(tensor1 tensor2)`
Multiplication matricielle de deux tenseurs.

### `tensor_mul(tensor1 tensor2)`
Multiplication √©l√©ment par √©l√©ment de deux tenseurs.

### `tensor_mul_scalar(tensor scalar)`
Multiplication de tous les √©l√©ments d'un tenseur par un scalaire.

### `tensor_neg(tensor)`
Calcule la n√©gation de chaque √©l√©ment du tenseur.

### `tensor_reciprocal(tensor)`
Calcule l'inverse (1/x) de chaque √©l√©ment du tenseur.

### `tensor_sub(tensor1 tensor2)`
Soustraction √©l√©ment par √©l√©ment de deux tenseurs.

## Fonctions Tenseur - Cr√©ation

### `tensor_cat(tensors dim)`
Concat√®ne une liste de tenseurs le long de la dimension sp√©cifi√©e.

### `tensor_full(shape value)`
Cr√©e un tenseur de la forme sp√©cifi√©e rempli avec une valeur.

### `tensor_full_like(tensor fill_value)`
Cr√©e un tenseur de m√™me forme qu'un tenseur existant, rempli avec une valeur.

### `tensor_ones(shape)`
Cr√©e un tenseur de la forme sp√©cifi√©e rempli de uns.

### `tensor_randn(shape)`
Cr√©e un tenseur de la forme sp√©cifi√©e avec des valeurs al√©atoires normales.

### `tensor_randint(low high shape)`
Cr√©e un tenseur d'entiers al√©atoires entre low (inclus) et high (exclus).

### `tensor_create(thedata) ou torch_tensor(thedata)`
Cr√©e un tenseur √† partir de donn√©es LispE (listes, matrices).

### `tensor_zeros(shape)`
Cr√©e un tenseur de la forme sp√©cifi√©e rempli de z√©ros.

## Fonctions Tenseur - Information

### `tensor_item(tensor)`
Extrait la valeur scalaire d'un tenseur √† un seul √©l√©ment.

### `tensor_shape(tensor)`
Retourne la forme (dimensions) d'un tenseur.

### `tensor_size(tensor)`
Retourne la taille totale (nombre d'√©l√©ments) d'un tenseur.

### `tensor_to_list(tensor)`
Convertit un tenseur PyTorch en liste LispE.

## Fonctions Tenseur - Manipulation

### `tensor_clamp(tensor min_val max_val)`
Limite les valeurs d'un tenseur entre min_val et max_val.

### `tensor_contiguous(tensor)`
S'assure qu'un tenseur est stock√© de mani√®re contigu√´ en m√©moire.

### `tensor_cumsum(tensor dim)`
Calcule la somme cumulative le long d'une dimension.

### `tensor_gather(input dim index)`
Collecte des valeurs le long d'une dimension selon un tenseur d'indices.

### `tensor_masked_fill_(tensor mask value)`
Remplace les √©l√©ments du tenseur par une valeur o√π le masque est vrai.

### `tensor_reshape(tensor shape)`
Change la forme d'un tenseur sans modifier ses donn√©es.

### `tensor_select(tensor dim index)`
S√©lectionne une tranche le long d'une dimension √† un index sp√©cifique.

### `tensor_set_item(tensor indices value)`
D√©finit la valeur √† des indices sp√©cifiques dans un tenseur.

### `tensor_slice(tensor dim start end)`
Extrait une tranche d'un tenseur le long d'une dimension.

### `tensor_squeeze(tensor dim)`
Supprime les dimensions de taille 1 du tenseur.

### `tensor_transpose(tensor dim0 dim1)`
Transpose deux dimensions d'un tenseur.

### `tensor_triu(tensor diagonal)`
Retourne la partie triangulaire sup√©rieure d'un tenseur matrice.

### `tensor_unsqueeze(tensor dim)`
Ajoute une dimension de taille 1 √† la position sp√©cifi√©e.

## Fonctions Tenseur - Math√©matiques

### `tensor_acos(tensor)`
Calcule l'arc cosinus de chaque √©l√©ment du tenseur.

### `tensor_asin(tensor)`
Calcule l'arc sinus de chaque √©l√©ment du tenseur.

### `tensor_atan(tensor)`
Calcule l'arc tangente de chaque √©l√©ment du tenseur.

### `tensor_ceil(tensor)`
Arrondit chaque √©l√©ment du tenseur vers le haut (plafond).

### `tensor_cos(tensor)`
Calcule le cosinus de chaque √©l√©ment du tenseur.

### `tensor_cosh(tensor)`
Calcule le cosinus hyperbolique de chaque √©l√©ment du tenseur.

### `tensor_einsum(indices tensors)`
Effectue la sommation d'Einstein sur les tenseurs selon la notation sp√©cifi√©e.

### `tensor_exp(tensor)`
Calcule l'exponentielle de chaque √©l√©ment du tenseur.

### `tensor_floor(tensor)`
Arrondit chaque √©l√©ment du tenseur vers le bas (plancher).

### `tensor_linear(tensor1 tensor2)`
Applique une transformation lin√©aire (multiplication matricielle + biais optionnel).

### `tensor_log(tensor)`
Calcule le logarithme naturel de chaque √©l√©ment du tenseur.

### `tensor_log10(tensor)`
Calcule le logarithme en base 10 de chaque √©l√©ment du tenseur.

### `tensor_log2(tensor)`
Calcule le logarithme en base 2 de chaque √©l√©ment du tenseur.

### `tensor_log_softmax(tensor dim)`
Applique log(softmax(x)) de mani√®re num√©riquement stable.

### `tensor_pow(tensor exponent)`
√âl√®ve chaque √©l√©ment du tenseur √† la puissance de l'exposant.

### `tensor_rms_norm(input weight (eps 1e-6))`
Applique la normalisation RMS (Root Mean Square) avec poids et epsilon.

### `tensor_round(tensor)`
Arrondit chaque √©l√©ment du tenseur √† l'entier le plus proche.

### `tensor_rsqrt(tensor)`
Calcule l'inverse de la racine carr√©e de chaque √©l√©ment.

### `tensor_sin(tensor)`
Calcule le sinus de chaque √©l√©ment du tenseur.

### `tensor_sinh(tensor)`
Calcule le sinus hyperbolique de chaque √©l√©ment du tenseur.

### `tensor_sqrt(tensor)`
Calcule la racine carr√©e de chaque √©l√©ment du tenseur.

### `tensor_tan(tensor)`
Calcule la tangente de chaque √©l√©ment du tenseur.

## Fonctions Tenseur - R√©duction

### `tensor_argmax(tensor dim (keepdim true))`
Retourne les indices des valeurs maximales le long d'une dimension.

### `tensor_max(tensor)`
Retourne la valeur maximale du tenseur.

### `tensor_mean(tensor)`
Calcule la moyenne de tous les √©l√©ments du tenseur.

### `tensor_mean_dim(tensor dim)`
Calcule la moyenne le long d'une dimension sp√©cifique.

### `tensor_min(tensor)`
Retourne la valeur minimale du tenseur.

### `tensor_std(tensor)`
Calcule l'√©cart-type de tous les √©l√©ments du tenseur.

### `tensor_sum(tensor)`
Calcule la somme de tous les √©l√©ments du tenseur.

## Fonctions Tokenisation

### `torch_decode(tokenizer token_ids)`
D√©code une liste d'IDs de tokens en texte avec un tokenizer.

### `torch_encode(tokenizer text)`
Encode un texte en liste d'IDs de tokens avec un tokenizer.

### `torch_pad_sequences(sequences max_length pad_token)`
Remplit des s√©quences pour qu'elles aient toutes la m√™me longueur.

### `torch_sentencepiece_tokenizer(model_path)`
Cr√©e un tokenizer SentencePiece √† partir d'un mod√®le pr√©-entra√Æn√©.

### `torch_simple_tokenizer()`
Cr√©e un tokenizer simple bas√© sur les espaces.

### `torch_train_sentencepiece(input_file model_prefix vocab_size model_type)`
Entra√Æne un nouveau mod√®le SentencePiece sur un fichier d'entr√©e.

### `torch_vocab_size(tokenizer)`
Retourne la taille du vocabulaire d'un tokenizer.

## Fonctions Tucker Decomposition

### `torch_khatri_rao_product(A B)`
Calcule le produit de Khatri-Rao de deux matrices.

### `torch_tucker_compression_ratio(original_shape core_shape factor_shapes)`
Calcule le ratio de compression d'une d√©composition de Tucker.

### `torch_tucker_decomposition(tensor rank (max_iter 100) (tol 1e-6))`
Effectue la d√©composition de Tucker d'un tenseur avec rang et param√®tres.

### `torch_tucker_reconstruct(core factors)`
Reconstruit un tenseur √† partir de son c≈ìur et facteurs de d√©composition Tucker.

## Fonctions Utilitaires

### `tensor_in_memory()`
Retourne des informations sur les tenseurs actuellement en m√©moire.

---

Ce manuel couvre les aspects essentiels de la librairie LispE PyTorch. Pour des cas d'usage sp√©cifiques ou des fonctionnalit√©s avanc√©es, consultez les exemples dans le d√©p√¥t et la documentation des fonctions individuelles.